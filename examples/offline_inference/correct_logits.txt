decoder layer hidden_states before attention tensor([ 0.1846, -0.6484,  0.0566,  0.2871, -0.3125, -0.0021, -0.2578, -0.0374,
         0.3164, -0.1514], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.3301,  0.6680,  0.2139, -0.2578, -0.3535, -0.2129, -0.1436, -0.2373,
        -0.1504, -0.1914], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.2598,  0.4766,  0.0645, -0.0713, -0.2910, -0.3613, -0.1270, -0.0894,
         0.0581, -0.0242], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.8203,  0.7344, -0.4531, -0.5781,  0.2871,  0.8008, -0.8477, -1.3047,
         0.6094,  0.3887], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.4570,  1.1328, -0.1621, -0.3320, -0.1846,  1.0625, -0.3594, -0.6758,
         0.4102,  0.2061], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.3535,  1.8984,  0.3535,  0.2051,  0.3965, -0.4121, -1.4297,  0.4883,
         0.6914, -0.6328], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.3379,  1.6250,  0.0679, -0.1904, -0.0571,  0.3809, -0.5312, -0.3320,
         0.5703, -0.0247], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.1025,  1.5625, -0.3867,  0.1289,  0.0505, -0.7812,  0.2217,  0.1045,
        -0.2754, -0.6953], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.2051,  1.5547, -0.0317, -0.0933,  0.0352,  0.2852, -0.2500, -0.1914,
         0.2578, -0.1299], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.5508, -0.2617,  0.3281,  0.6445,  0.3496, -0.2617,  0.8477, -0.3086,
         0.0608, -0.0187], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.3848,  1.4219,  0.0098, -0.0162,  0.1719,  0.2109, -0.1377, -0.2324,
         0.3066, -0.1631], device='xla:0')
decoder layer hidden_states after attention tensor([-0.1211,  1.5781, -0.4531,  0.2988,  0.4492, -0.7734,  1.1328, -0.5430,
        -1.3594, -0.3301], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.3691,  1.6953, -0.1006,  0.0282,  0.2852, -0.0471,  0.0884, -0.3730,
         0.1172, -0.2637], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.0688, -0.4570, -0.5000, -0.3867,  0.0058, -0.4160, -1.4609, -0.8711,
         0.8047,  0.2109], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.3164,  1.3203, -0.1377, -0.0425,  0.1924, -0.1167, -0.1465, -0.4316,
         0.2354, -0.1602], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.8789,  1.2734, -0.2285, -0.6914, -1.0078,  1.3672,  0.4121, -0.8477,
        -0.1240, -0.9688], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.3809,  1.2656, -0.1201, -0.1680, -0.0752,  0.2715, -0.1094, -0.4766,
         0.1826, -0.3711], device='xla:0')
decoder layer hidden_states after attention tensor([-0.8789,  0.1270,  0.3242, -0.2871,  0.2041,  0.8516,  0.3027, -0.5039,
        -0.7578, -0.7539], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.2354,  1.0312, -0.0168, -0.2334, -0.0532,  0.7461, -0.0693, -0.4805,
         0.0334, -0.5664], device='xla:0')
decoder layer hidden_states after attention tensor([-1.4453,  0.5273,  1.0547,  0.6133, -0.4395,  0.0432, -0.2617, -0.9258,
         0.6719,  0.2324], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.0369,  1.1016,  0.3164, -0.1182, -0.1670,  0.5391, -0.1748, -0.7812,
         0.1602, -0.4570], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.3145,  0.4004,  0.6172, -0.1709,  0.0510, -0.6250,  0.4863,  0.5117,
        -0.8984, -0.7344], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.0459,  1.0547,  0.4180, -0.1494, -0.1201,  0.3105, -0.0303, -0.6680,
        -0.0869, -0.5312], device='xla:0')
decoder layer hidden_states after attention tensor([-0.0469,  0.2402, -1.5547, -0.4824, -0.9492, -1.0469,  0.9336, -0.2070,
         0.5273, -0.5039], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.0732,  1.1406, -0.1108, -0.2910, -0.3125,  0.0209,  0.4199, -0.5547,
         0.0483, -0.7695], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.4629, -0.0615,  0.5430,  0.3047, -1.2031, -0.0752,  0.7266, -0.9180,
         0.4648,  0.0679], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.1069,  0.9570,  0.0693, -0.1855, -0.4355,  0.0869,  0.5820, -0.5781,
         0.1475, -0.5312], device='xla:0')
decoder layer hidden_states after attention tensor([-0.2109, -0.2285, -0.3164,  0.5391,  0.4336, -1.9766, -0.0918,  0.2041,
         0.9883,  0.5898], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.0635,  0.9609, -0.0469, -0.0854, -0.4492, -0.7734,  0.7969, -0.6953,
         0.4258, -0.4980], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.8711, -0.3926,  0.5078,  0.0459,  0.1221, -0.8008,  0.5195, -1.1719,
         0.5469, -0.0369], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.1934,  0.8672,  0.1133, -0.1260, -0.3867, -0.5352,  1.1875, -0.6875,
         0.5859, -0.5430], device='xla:0')
decoder layer hidden_states after attention tensor([-0.6797,  1.0703, -0.5781,  1.0156, -0.4043,  0.9297, -0.2617,  0.5703,
         1.8438,  1.2031], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.1162,  0.9844, -0.0645,  0.0977, -0.4316, -0.3223,  0.8789, -0.5859,
         0.9648, -0.0214], device='xla:0')
decoder layer hidden_states after attention tensor([-0.2295,  0.0493, -0.7969, -0.2832,  0.2373,  0.6055, -0.0249,  0.4590,
        -0.6133, -0.1123], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.0820,  1.2578, -0.3262,  0.0058, -0.5859, -0.3691,  0.8359, -0.7500,
         0.7539,  0.0562], device='xla:0')
decoder layer hidden_states after attention tensor([-0.4824,  1.9375, -0.1611, -0.2910, -0.0476,  0.3418,  0.3887, -0.5781,
        -0.0457,  0.1738], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.0223,  1.4141, -0.2871, -0.0703, -0.4219, -0.1748,  0.6719, -0.5312,
         0.6562,  0.0344], device='xla:0')
decoder layer hidden_states after attention tensor([-0.5820,  0.5977, -0.6133,  0.2930,  0.5703, -0.5039,  0.5703, -0.7305,
        -0.8906,  0.3203], device='xla:0')
decoder layer hidden_states before attention tensor([-0.0928,  1.7578, -0.4844, -0.0270, -0.5078, -0.3789,  0.7930, -1.0703,
         0.5352,  0.0903], device='xla:0')
decoder layer hidden_states after attention tensor([ 1.3672,  0.4785,  0.0513,  0.5430,  1.2578,  0.5117, -1.0625, -0.1177,
         0.7461, -0.0659], device='xla:0')
decoder layer hidden_states before attention tensor([ 0.1875,  1.6719, -0.4219,  0.0393, -0.1758, -0.3535,  0.5078, -0.9805,
         0.7422,  0.0586], device='xla:0')
decoder layer hidden_states after attention tensor([-1.1797,  0.1699, -0.9883,  1.0469,  0.0123, -0.0505, -0.0364,  0.0635,
         0.0486,  0.9688], device='xla:0')
decoder layer hidden_states before attention tensor([-0.0117,  1.8047, -0.4648,  0.2188, -0.1650, -0.3262,  0.3750, -0.8281,
         0.6406,  0.2812], device='xla:0')
decoder layer hidden_states after attention tensor([-0.9023,  0.6055, -0.4531, -0.6641, -0.4023,  1.9062, -0.4629, -0.0155,
        -0.8164,  1.5781], device='xla:0')
decoder layer hidden_states before attention tensor([-0.1641,  2.0781, -0.7148,  0.2236, -0.2139,  0.2773,  0.1875, -0.7188,
         0.4648,  0.4570], device='xla:0')
decoder layer hidden_states after attention tensor([-1.1094,  0.2285,  1.7266,  0.1147,  0.8594,  1.2031,  1.2812, -0.2852,
        -1.0391, -0.5430], device='xla:0')
decoder layer hidden_states before attention tensor([-0.4707,  2.4531, -0.4453,  0.1514, -0.1084,  0.4609,  0.3867, -1.1016,
         0.2539,  0.3496], device='xla:0')
decoder layer hidden_states after attention tensor([ 1.0547,  0.5000,  0.7891, -0.1953, -0.4355, -0.2910, -0.1230, -0.2949,
         0.8047,  0.9258], device='xla:0')
decoder layer hidden_states before attention tensor([-0.5195,  2.9219, -0.3965,  0.2910, -0.2715,  0.4414,  0.4844, -1.4844,
         0.4609,  0.7227], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.7695,  1.2812,  1.1094, -0.7188,  0.6133, -0.9180,  0.0500, -0.2754,
        -0.0615, -1.3828], device='xla:0')
decoder layer hidden_states before attention tensor([-0.3184,  3.3281, -0.0684,  0.1426, -0.1025, -0.0188,  0.2598, -0.8672,
         0.3184,  0.3262], device='xla:0')
decoder layer hidden_states after attention tensor([-1.0078,  0.2676, -0.6289,  0.8047,  0.5977,  0.2988,  0.4980,  0.0747,
         0.3379, -0.5664], device='xla:0')
decoder layer hidden_states before attention tensor([-0.7305,  3.1250, -0.1641, -0.5273, -0.3047,  0.0251, -0.1270, -0.3105,
         1.1641,  0.7539], device='xla:0')
decoder layer hidden_states after attention tensor([-1.8594,  1.0938,  0.0226, -1.0781,  0.5547, -0.6406, -1.9141,  0.3652,
        -1.0391,  0.0535], device='xla:0')
decoder layer hidden_states before attention tensor([-1.0078,  3.8594, -0.0201, -0.4883, -0.0258, -0.2197, -0.0913, -0.2832,
         0.7266,  0.7617], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.9570,  0.0684,  0.0060, -0.0410, -0.3535,  0.1875, -0.6211, -0.3613,
        -0.1172,  0.1660], device='xla:0')
INFO 05-03 04:05:50 [kv_cache_utils.py:639] GPU KV cache size: 933,728 tokens
INFO 05-03 04:05:50 [kv_cache_utils.py:642] Maximum concurrency for 32,768 tokens per request: 28.50x
INFO 05-03 04:05:50 [core.py:161] init engine (profile, create kv cache, warmup model) took 28.26 seconds
INFO 05-03 04:05:50 [core_client.py:442] Core engine process 0 ready.
Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 128.24it/s]
Processed prompts:   0%|                                                                                | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]decoder layer hidden_states before attention tensor([ 0.4961,  0.1826,  0.6367,  0.1787, -0.2852,  0.4062,  0.0270,  0.0659,
         0.5664, -0.2793], device='xla:0')
/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/jax/_src/cloud_tpu_init.py:82: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c "echo always > /sys/kernel/mm/transparent_hugepage/enabled")
  warnings.warn(
decoder layer hidden_states after attention tensor([-0.0513, -0.0820, -0.0044,  0.0762,  0.0674, -0.0615,  0.0106,  0.0732,
        -0.0457, -0.0019], device='xla:0')
decoder layer hidden_states before attention tensor([-0.2852,  0.1797,  0.0728,  0.0173, -0.2871, -0.2393, -0.0898,  0.3340,
         0.0179, -0.0195], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.0038,  0.0613, -0.0894, -0.0869,  0.0469,  0.1206, -0.0479, -0.0004,
        -0.0957, -0.0172], device='xla:0')
decoder layer hidden_states before attention tensor([-0.0181,  0.1079, -0.0148,  0.0315,  0.0093, -0.0018,  0.0244,  0.0469,
        -0.0393, -0.0103], device='xla:0')
decoder layer hidden_states after attention tensor([ 0.0349,  0.0569, -0.0537, -0.0016, -0.0176, -0.1113,  0.0074,  0.0187,
        -0.0061, -0.0280], device='xla:0')
decoder layer hidden_states before attention tensor([-0.0168,  0.1064,  0.0019,  0.0168,  0.0008, -0.0019,  0.0077,  0.0278,
        -0.0154, -0.0072], device='xla:0')
decoder layer hidden_states after attention ERROR 05-03 04:06:01 [core.py:402] EngineCore encountered a fatal error.
ERROR 05-03 04:06:01 [core.py:402] Traceback (most recent call last):
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/vllm/vllm/v1/engine/core.py", line 393, in run_engine_core
ERROR 05-03 04:06:01 [core.py:402]     engine_core.run_busy_loop()
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/vllm/vllm/v1/engine/core.py", line 417, in run_busy_loop
ERROR 05-03 04:06:01 [core.py:402]     self._process_engine_step()
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/vllm/vllm/v1/engine/core.py", line 442, in _process_engine_step
ERROR 05-03 04:06:01 [core.py:402]     outputs = self.step_fn()
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/vllm/vllm/v1/engine/core.py", line 205, in step
ERROR 05-03 04:06:01 [core.py:402]     output = self.model_executor.execute_model(scheduler_output)
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/vllm/vllm/v1/executor/abstract.py", line 86, in execute_model
ERROR 05-03 04:06:01 [core.py:402]     output = self.collective_rpc("execute_model",
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/vllm/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 05-03 04:06:01 [core.py:402]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/vllm/vllm/utils.py", line 2484, in run_method
ERROR 05-03 04:06:01 [core.py:402]     return func(*args, **kwargs)
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/vllm/vllm/v1/worker/tpu_worker.py", line 222, in execute_model
ERROR 05-03 04:06:01 [core.py:402]     output = self.model_runner.execute_model(scheduler_output)
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
ERROR 05-03 04:06:01 [core.py:402]     return func(*args, **kwargs)
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/vllm/vllm/v1/worker/tpu_model_runner.py", line 786, in execute_model
ERROR 05-03 04:06:01 [core.py:402]     hidden_states = self.model(
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
ERROR 05-03 04:06:01 [core.py:402]     return self._call_impl(*args, **kwargs)
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
ERROR 05-03 04:06:01 [core.py:402]     return forward_call(*args, **kwargs)
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/vllm/vllm/model_executor/models/qwen2.py", line 471, in forward
ERROR 05-03 04:06:01 [core.py:402]     hidden_states = self.model(input_ids, positions, intermediate_tensors,
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
ERROR 05-03 04:06:01 [core.py:402]     return self._call_impl(*args, **kwargs)
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
ERROR 05-03 04:06:01 [core.py:402]     return forward_call(*args, **kwargs)
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward
ERROR 05-03 04:06:01 [core.py:402]     hidden_states, residual = layer(
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
ERROR 05-03 04:06:01 [core.py:402]     return self._call_impl(*args, **kwargs)
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
ERROR 05-03 04:06:01 [core.py:402]     return forward_call(*args, **kwargs)
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/vllm/vllm/model_executor/models/qwen2.py", line 250, in forward
ERROR 05-03 04:06:01 [core.py:402]     print("decoder layer hidden_states after attention", hidden_states.ravel()[:10])
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/_tensor.py", line 590, in __repr__
ERROR 05-03 04:06:01 [core.py:402]     return torch._tensor_str._str(self, tensor_contents=tensor_contents)
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/_tensor_str.py", line 726, in _str
ERROR 05-03 04:06:01 [core.py:402]     return _str_intern(self, tensor_contents=tensor_contents)
ERROR 05-03 04:06:01 [core.py:402]   File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/_tensor_str.py", line 462, in _str_intern
ERROR 05-03 04:06:01 [core.py:402]     self = self.to("cpu")
ERROR 05-03 04:06:01 [core.py:402] RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: Error loading program: Attempting to reserve 3.56G at the bottom of memory. That was not possible. There are 3.43G free, 0B reserved, and 3.43G reservable.
Process EngineCore_0:
Traceback (most recent call last):
  File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/wenxindong_google_com/vllm/vllm/v1/engine/core.py", line 404, in run_engine_core
    raise e
  File "/home/wenxindong_google_com/vllm/vllm/v1/engine/core.py", line 393, in run_engine_core
    engine_core.run_busy_loop()
  File "/home/wenxindong_google_com/vllm/vllm/v1/engine/core.py", line 417, in run_busy_loop
    self._process_engine_step()
  File "/home/wenxindong_google_com/vllm/vllm/v1/engine/core.py", line 442, in _process_engine_step
    outputs = self.step_fn()
  File "/home/wenxindong_google_com/vllm/vllm/v1/engine/core.py", line 205, in step
    output = self.model_executor.execute_model(scheduler_output)
  File "/home/wenxindong_google_com/vllm/vllm/v1/executor/abstract.py", line 86, in execute_model
    output = self.collective_rpc("execute_model",
  File "/home/wenxindong_google_com/vllm/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/home/wenxindong_google_com/vllm/vllm/utils.py", line 2484, in run_method
    return func(*args, **kwargs)
  File "/home/wenxindong_google_com/vllm/vllm/v1/worker/tpu_worker.py", line 222, in execute_model
    output = self.model_runner.execute_model(scheduler_output)
  File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/wenxindong_google_com/vllm/vllm/v1/worker/tpu_model_runner.py", line 786, in execute_model
    hidden_states = self.model(
  File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wenxindong_google_com/vllm/vllm/model_executor/models/qwen2.py", line 471, in forward
    hidden_states = self.model(input_ids, positions, intermediate_tensors,
  File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wenxindong_google_com/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward
    hidden_states, residual = layer(
  File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wenxindong_google_com/vllm/vllm/model_executor/models/qwen2.py", line 250, in forward
    print("decoder layer hidden_states after attention", hidden_states.ravel()[:10])
  File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/_tensor.py", line 590, in __repr__
    return torch._tensor_str._str(self, tensor_contents=tensor_contents)
  File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/_tensor_str.py", line 726, in _str
    return _str_intern(self, tensor_contents=tensor_contents)
  File "/home/wenxindong_google_com/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/_tensor_str.py", line 462, in _str_intern
    self = self.to("cpu")
RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: Error loading program: Attempting to reserve 3.56G at the bottom of memory. That was not possible. There are 3.43G free, 0B reserved, and 3.43G reservable.
Traceback (most recent call last):
  File "/home/wenxindong_google_com/vllm/examples/offline_inference/tpu.py", line 37, in <module>
    main()
  File "/home/wenxindong_google_com/vllm/examples/offline_inference/tpu.py", line 26, in main
    outputs = llm.generate(prompts, sampling_params)
  File "/home/wenxindong_google_com/vllm/vllm/utils.py", line 1197, in inner
    return fn(*args, **kwargs)
  File "/home/wenxindong_google_com/vllm/vllm/entrypoints/llm.py", line 472, in generate
    outputs = self._run_engine(use_tqdm=use_tqdm)
  File "/home/wenxindong_google_com/vllm/vllm/entrypoints/llm.py", line 1455, in _run_engine
    step_outputs = self.llm_engine.step()
  File "/home/wenxindong_google_com/vllm/vllm/v1/engine/llm_engine.py", line 220, in step
    outputs = self.engine_core.get_output()
  File "/home/wenxindong_google_com/vllm/vllm/v1/engine/core_client.py", line 565, in get_output
    raise self._format_exception(outputs) from None
vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.